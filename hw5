#1

import numpy as np

# Data
t_c = np.array([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])
t_u = np.array([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])

# Normalize t_u
t_un = 0.1 * t_u

# Initialize the model parameters
w1 = np.ones(1)
w2 = np.ones(1)
b = np.zeros(1)

# Define the model
def model(t_u, w1, w2, b):
    return w2 * t_u ** 2 + w1 * t_u + b

# Define the loss function
def loss_fn(y, y_pred):
    return np.mean((y_pred - y) ** 2)
def training_loop(n_epochs, learning_rate, params, t_u_train, t_c_train):
    w1, w2, b = params

    for epoch in range(1, n_epochs + 1):
        t_p_train = model(t_u_train, w1, w2, b)
        loss = loss_fn(t_c_train, t_p_train)

        # Compute gradients
        grad_w1 = np.mean(2 * (t_p_train - t_c_train) * t_u_train)
        grad_w2 = np.mean(2 * (t_p_train - t_c_train) * t_u_train ** 2)
        grad_b = np.mean(2 * (t_p_train - t_c_train))

        # Update parameters
        w1 -= learning_rate * grad_w1
        w2 -= learning_rate * grad_w2
        b -= learning_rate * grad_b

        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Loss {loss}")

    return w1, w2, b
learning_rates = [0.1, 0.01, 0.001, 0.0001]
n_epochs = 5000

for lr in learning_rates:
    print(f"\nTraining with learning rate: {lr}")
    w1 = np.ones(1)
    w2 = np.ones(1)
    b = np.zeros(1)
    params = (w1, w2, b)

    trained_params = training_loop(n_epochs, lr, params, t_un, t_c)
import matplotlib.pyplot as plt

# Define the linear model for comparison
def linear_model(t_u, w, b):
    return w * t_u + b

# Train the linear model for comparison
def training_loop_linear(n_epochs, learning_rate, params, t_u_train, t_c_train):
    w, b = params

    for epoch in range(1, n_epochs + 1):
        t_p_train = linear_model(t_u_train, w, b)
        loss = loss_fn(t_c_train, t_p_train)

        # Compute gradients
        grad_w = np.mean(2 * (t_p_train - t_c_train) * t_u_train)
        grad_b = np.mean(2 * (t_p_train - t_c_train))

        # Update parameters
        w -= learning_rate * grad_w
        b -= learning_rate * grad_b

        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Loss {loss}")

    return w, b

# Training linear model
w_lin = np.ones(1)
b_lin = np.zeros(1)
params_lin = (w_lin, b_lin)
trained_params_lin = training_loop_linear(n_epochs, 0.01, params_lin, t_un, t_c)

# Plot the results
plt.figure(figsize=(10, 6))

# Non-linear model
t_p_nonlinear = model(t_un, trained_params[0], trained_params[1], trained_params[2])
plt.plot(t_u, t_p_nonlinear, label="Non-linear model", color='r')

# Linear model
t_p_linear = linear_model(t_un, trained_params_lin[0], trained_params_lin[1])
plt.plot(t_u, t_p_linear, label="Linear model", color='b')

# Original data
plt.scatter(t_u, t_c, label="Data", color='g')
plt.xlabel("Measurement")
plt.ylabel("Temperature (Celsius)")
plt.legend()
plt.show()
#2
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('Housing.csv')

# Select relevant features and target variable
features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
target = 'price'

# Split the data into features (X) and target (y)
X = data[features].values
y = data[target].values

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and validation sets (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize model parameters
def initialize_parameters(n_features):
    w = np.zeros(n_features)
    b = 0
    return w, b

# Define the linear regression model
def linear_model(X, w, b):
    return np.dot(X, w) + b

# Define the loss function (mean squared error)
def loss_fn(y_true, y_pred):
    return np.mean((y_pred - y_true) ** 2)

# Implement the training loop
def training_loop(n_epochs, learning_rate, X_train, y_train, X_val, y_val):
    n_features = X_train.shape[1]
    w, b = initialize_parameters(n_features)
    
    for epoch in range(1, n_epochs + 1):
        # Predictions
        y_pred_train = linear_model(X_train, w, b)
        y_pred_val = linear_model(X_val, w, b)
        
        # Calculate loss
        train_loss = loss_fn(y_train, y_pred_train)
        val_loss = loss_fn(y_val, y_pred_val)
        
        # Calculate gradients
        grad_w = np.dot(X_train.T, (y_pred_train - y_train)) / len(y_train)
        grad_b = np.mean(y_pred_train - y_train)
        
        # Update parameters
        w -= learning_rate * grad_w
        b -= learning_rate * grad_b
        
        # Report loss every 500 epochs
        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}")
    
    return w, b

# Train the model with different learning rates
learning_rates = [0.1, 0.01, 0.001, 0.0001]
n_epochs = 5000

for lr in learning_rates:
    print(f"\nTraining with learning rate: {lr}")
    w, b = training_loop(n_epochs, lr, X_train, y_train, X_val, y_val)
#3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('Housing.csv')
# Check for non-numeric columns
print(data.dtypes)

# Convert non-numeric columns to numeric or drop them
data = pd.get_dummies(data, drop_first=True)  # One-hot encode categorical variables

# Select all features and target variable
features = data.columns.drop('price')
target = 'price'

# Split the data into features (X) and target (y)
X = data[features].values
y = data[target].values

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and validation sets (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize model parameters
def initialize_parameters(n_features):
    w = np.zeros(n_features)
    b = 0
    return w, b

# Define the linear regression model
def linear_model(X, w, b):
    return np.dot(X, w) + b

# Define the loss function (mean squared error)
def loss_fn(y_true, y_pred):
    return np.mean((y_pred - y_true) ** 2)

# Implement the training loop
def training_loop(n_epochs, learning_rate, X_train, y_train, X_val, y_val):
    n_features = X_train.shape[1]
    w, b = initialize_parameters(n_features)
    
    for epoch in range(1, n_epochs + 1):
        # Predictions
        y_pred_train = linear_model(X_train, w, b)
        y_pred_val = linear_model(X_val, w, b)
        
        # Calculate loss
        train_loss = loss_fn(y_train, y_pred_train)
        val_loss = loss_fn(y_val, y_pred_val)
        
        # Calculate gradients
        grad_w = np.dot(X_train.T, (y_pred_train - y_train)) / len(y_train)
        grad_b = np.mean(y_pred_train - y_train)
        
        # Update parameters
        w -= learning_rate * grad_w
        b -= learning_rate * grad_b
        
        # Report loss every 500 epochs
        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}")
    
    return w, b

# Train the model with different learning rates
learning_rates = [0.1, 0.01, 0.001, 0.0001]
n_epochs = 5000

for lr in learning_rates:
    print(f"\nTraining with learning rate: {lr}")
    w, b = training_loop(n_epochs, lr, X_train, y_train, X_val, y_val)

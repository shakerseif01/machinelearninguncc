import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('cancer.csv')

# Inspect the first few rows of the dataset to understand its structure
print(data.head())
print(data.columns)

# Replace 'diagnosis' with the actual target column name if it's different
target_column = 'diagnosis'  # Change this to the correct column name

# Split the data into features and target
X = data.drop(columns=[target_column])
y = data[target_column]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize lists to store the results
accuracy_list = []
precision_list = []
recall_list = []
n_components_list = range(1, X.shape[1] + 1)

# Iterate over different numbers of principal components
for n_components in n_components_list:
    # Apply PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
    
    # Train the SVM classifier
    svm = SVC(kernel='linear')  # You can change the kernel to 'rbf', 'poly', etc.
    svm.fit(X_train, y_train)
    
    # Make predictions
    y_pred = svm.predict(X_test)
    
    # Calculate the metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, pos_label='M')  # Assuming 'M' is for malignant
    recall = recall_score(y_test, y_pred, pos_label='M')
    
    # Store the results
    accuracy_list.append(accuracy)
    precision_list.append(precision)
    recall_list.append(recall)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(n_components_list, accuracy_list, label='Accuracy')
plt.plot(n_components_list, precision_list, label='Precision')
plt.plot(n_components_list, recall_list, label='Recall')
plt.xlabel('Number of Principal Components')
plt.ylabel('Score')
plt.legend()
plt.title('SVM Classifier Performance with Different Numbers of Principal Components')
plt.show()

# Exploring different kernels
kernels = ['linear', 'rbf', 'poly', 'sigmoid']
kernel_accuracy = []

# Determine the optimal number of components based on the plots (you can choose based on previous results)
optimal_n_components = accuracy_list.index(max(accuracy_list)) + 1

for kernel in kernels:
    # Apply PCA with optimal number of components
    pca = PCA(n_components=optimal_n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
    
    # Train the SVM classifier
    svm = SVC(kernel=kernel)
    svm.fit(X_train, y_train)
    
    # Make predictions
    y_pred = svm.predict(X_test)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    kernel_accuracy.append(accuracy)

# Plot the kernel comparison
plt.figure(figsize=(8, 4))
plt.bar(kernels, kernel_accuracy)
plt.xlabel('Kernel')
plt.ylabel('Accuracy')
plt.title('Comparison of SVM Kernels')
plt.show()
#2
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('cancer.csv')

# Inspect the first few rows of the dataset to understand its structure
print(data.head())
print(data.columns)

# Replace 'target' with the actual target column name
target_column = 'target'  # Change this to the correct column name for regression task

# Split the data into features and target
X = data.drop(columns=[target_column])
y = data[target_column]

# Handle missing values
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Ensure the data is dense and float type
X_imputed = np.asarray(X_imputed, dtype=np.float64)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Initialize lists to store the results
n_components_list = range(1, X.shape[1] + 1)
svr_results = []
ridge_results = []

# Iterate over different numbers of principal components
for n_components in n_components_list:
    # Apply PCA
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
    
    # Train SVR with linear kernel
    svr = SVR(kernel='linear')
    svr.fit(X_train, y_train)
    y_pred_svr = svr.predict(X_test)
    mse_svr = mean_squared_error(y_test, y_pred_svr)
    svr_results.append(mse_svr)
    
    # Train Ridge regression
    ridge = Ridge()
    ridge.fit(X_train, y_train)
    y_pred_ridge = ridge.predict(X_test)
    mse_ridge = mean_squared_error(y_test, y_pred_ridge)
    ridge_results.append(mse_ridge)

# Plot the MSE for SVR and Ridge regression
plt.figure(figsize=(12, 6))
plt.plot(n_components_list, svr_results, label='SVR')
plt.plot(n_components_list, ridge_results, label='Ridge Regression')
plt.xlabel('Number of Principal Components')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.title('SVR vs Ridge Regression Performance with Different Numbers of Principal Components')
plt.show()

# Exploring different kernels for SVR
kernels = ['linear', 'rbf', 'poly', 'sigmoid']
kernel_results = {kernel: [] for kernel in kernels}

for kernel in kernels:
    for n_components in n_components_list:
        # Apply PCA
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X_scaled)
        
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)
        
        # Train SVR with the given kernel
        svr = SVR(kernel=kernel)
        svr.fit(X_train, y_train)
        y_pred = svr.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        kernel_results[kernel].append(mse)

# Plot the kernel comparison
plt.figure(figsize=(12, 6))
for kernel in kernels:
    plt.plot(n_components_list, kernel_results[kernel], label=f'SVR with {kernel} kernel')
plt.xlabel('Number of Principal Components')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.title('Comparison of SVR Kernels')
plt.show()
